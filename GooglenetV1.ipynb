{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sarvesh-Kesharwani/Deep-Learning/blob/main/GooglenetV1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIYdn1woOS1n",
        "outputId": "4025184b-42a8-4589-92c9-e147151cfb42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'models' already exists and is not an empty directory.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.1+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (16.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q ivy\n",
        "\n",
        "!git clone https://github.com/unifyai/models.git\n",
        "!cd models && python3 -m pip install -q -e .\n",
        "\n",
        "!python3 -m pip install torchvision\n",
        "\n",
        "exit()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ivy\n",
        "import torch\n",
        "import ivy_models\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "DIhwVfd0RNQY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inception model implementation"
      ],
      "metadata": {
        "id": "udG7M1wNiKy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Building the initial Convolutional Block\n",
        "class ConvBlock(ivy.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        super(ConvBlock, self).__init__()\n",
        "\n",
        "    def _build(self, *args, **kwargs):\n",
        "        self.conv = ivy.Conv2D(self.in_channels, self.out_channels, self.kernel_size, self.stride, self.padding, with_bias=False)\n",
        "        self.bn = ivy.BatchNorm2D(self.out_channels)\n",
        "        self.activation = ivy.ReLU()\n",
        "\n",
        "    def _forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.activation(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "IK1u-ViuRNL9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Inception(ivy.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        num1x1,\n",
        "        num3x3_reduce,\n",
        "        num3x3,\n",
        "        num5x5_reduce,\n",
        "        num5x5,\n",
        "        pool_proj,\n",
        "    ):\n",
        "        self.in_channels = in_channels\n",
        "        self.num1x1 = num1x1\n",
        "        self.num3x3_reduce = num3x3_reduce\n",
        "        self.num3x3 = num3x3\n",
        "        self.num5x5_reduce = num5x5_reduce\n",
        "        self.num5x5 = num5x5\n",
        "        self.pool_proj = pool_proj\n",
        "        super(Inception, self).__init__()\n",
        "\n",
        "    def _build(self, *args, **kwargs):\n",
        "        self.block1 = ivy.Sequential(ConvBlock(self.in_channels, self.num1x1, kernel_size=[1, 1], stride=1, padding=0)\n",
        "                                     )\n",
        "        self.block2 = ivy.Sequential(ConvBlock(self.in_channels, self.num3x3_reduce, kernel_size=[1, 1], stride=1, padding=0),\n",
        "                                    ConvBlock(self.num3x3_reduce, self.num3x3, kernel_size=[3, 3], stride=1, padding=1)\n",
        "                                    )\n",
        "        self.block3 = ivy.Sequential(ConvBlock(self.in_channels, self.num5x5_reduce, kernel_size=[1, 1], stride=1, padding=0),\n",
        "                                    ConvBlock(self.num5x5_reduce, self.num5x5, kernel_size=[3, 3], stride=1, padding=1)\n",
        "                                    )\n",
        "        self.block4 = ivy.Sequential(ivy.MaxPool2D(3, 1, 1),\n",
        "                                    ConvBlock(self.in_channels, self.pool_proj, kernel_size=[1, 1], stride=1, padding=0)\n",
        "                                    )\n",
        "\n",
        "    def _forward(self, x):\n",
        "        block1 = self.block1(x)\n",
        "        block2 = self.block2(x)\n",
        "        block3 = self.block3(x)\n",
        "        block4 = self.block4(x)\n",
        "\n",
        "        return ivy.concat([block1, block2, block3, block4], axis=3)"
      ],
      "metadata": {
        "id": "BnaGLv7hRT7S"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Auxiliary(ivy.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        self.in_channels = in_channels\n",
        "        self.num_classes = num_classes\n",
        "        super(Auxiliary, self).__init__()\n",
        "\n",
        "    def _build(self, *args, **kwargs):\n",
        "        self.pool = ivy.AvgPool2D((5, 5), 3, 0)  # ivy.Shape(1, 4, 4, 512)\n",
        "        self.bn = ivy.BatchNorm2D(128)\n",
        "        self.conv = ivy.Conv2D(self.in_channels, 128, [1,1], 1, 0, with_bias=False)\n",
        "        self.activation = ivy.ReLU()\n",
        "        self.fc1 = ivy.Linear(2048, 1024)\n",
        "        self.dropout = ivy.Dropout(0.7)\n",
        "        self.fc2 = ivy.Linear(1024, self.num_classes)\n",
        "\n",
        "    def _forward(self, x):\n",
        "        out = self.pool(x)\n",
        "        out = self.conv(out) # contains weights\n",
        "        out = self.activation(out)\n",
        "        out = ivy.flatten(out, start_dim=1)\n",
        "        out = self.fc1(out) # contains weights\n",
        "        out = self.activation(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out) # contains weights\n",
        "        return out"
      ],
      "metadata": {
        "id": "XYekmDl8RT2t"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GoogLeNet(ivy.Module):\n",
        "    def __init__(self, num_classes=1000, v: ivy.Container = None,):\n",
        "        if v is not None:\n",
        "            self.v = v\n",
        "        self.num_classes = num_classes\n",
        "        super(GoogLeNet, self).__init__(v=v)\n",
        "\n",
        "    def _build(self, *args, **kwargs):\n",
        "        self.conv1 = ConvBlock(3, 64, [7,7], 2, 3)\n",
        "        self.pool1 = ivy.MaxPool2D([3,3], 2, 1)\n",
        "        self.conv2 = ConvBlock(64, 64, [1,1], 1, 0,)\n",
        "        self.conv3 = ConvBlock(64, 192, [3,3], 1, 1)\n",
        "        self.pool3 = ivy.MaxPool2D(3, 2, 1)\n",
        "        self.inception3A = Inception(192, 64, 96, 128, 16, 32, 32)\n",
        "        self.inception3B = Inception(256, 128, 128, 192, 32, 96, 64)\n",
        "        self.pool4 = ivy.MaxPool2D(3, 2, 1)\n",
        "\n",
        "        self.inception4A = Inception(480, 192, 96, 208, 16, 48, 64)\n",
        "\n",
        "        # ivy.flatten()\n",
        "        self.aux4A = Auxiliary(512, self.num_classes)\n",
        "\n",
        "        self.inception4B = Inception(512, 160, 112, 224, 24, 64, 64)\n",
        "        self.inception4C = Inception(512, 128, 128, 256, 24, 64, 64)\n",
        "        self.inception4D = Inception(512, 112, 144, 288, 32, 64, 64)\n",
        "\n",
        "        self.aux4D = Auxiliary(528, self.num_classes)\n",
        "\n",
        "        self.inception4E = Inception(528, 256, 160, 320, 32, 128, 128)\n",
        "        self.pool5 = ivy.MaxPool2D(3, 2, 1)\n",
        "\n",
        "        self.inception5A = Inception(832, 256, 160, 320, 32, 128, 128)\n",
        "        self.inception5B = Inception(832, 384, 192, 384, 48, 128,128)\n",
        "        self.pool6 = ivy.AvgPool2D((7,7), 1, 0) # ((1, 1))\n",
        "\n",
        "        # ivy.flatten()\n",
        "        self.dropout = ivy.Dropout(0.4)\n",
        "        self.fc = ivy.Linear(1024, self.num_classes)\n",
        "\n",
        "\n",
        "    def _forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.pool1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.pool3(out)\n",
        "        out = self.inception3A(out)\n",
        "        out = self.inception3B(out)\n",
        "        out = self.pool4(out)\n",
        "        out = self.inception4A(out)\n",
        "\n",
        "        aux1 = self.aux4A(out)\n",
        "\n",
        "        out = self.inception4B(out)\n",
        "        out = self.inception4C(out)\n",
        "        out = self.inception4D(out)\n",
        "\n",
        "        aux2 = self.aux4D(out)\n",
        "\n",
        "        out = self.inception4E(out)\n",
        "        out = self.pool5(out)\n",
        "        out = self.inception5A(out)\n",
        "        out = self.inception5B(out)\n",
        "        out = self.pool6(out)\n",
        "        out = ivy.flatten(out, start_dim=1)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, aux1, aux2\n"
      ],
      "metadata": {
        "id": "RaBEdDZ1iSFt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up ivy compiler api key\n"
      ],
      "metadata": {
        "id": "_oHD5NQKiY42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir .ivy\n",
        "!echo -n gDNLm8VEO7QleBN5GdTcfMa7OHWBOWcyOiHWr2KI654= > .ivy/key.pem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWXhkvXXjx9o",
        "outputId": "3e7baa18-9f3b-482c-d54f-a9d6b986010d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘.ivy’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ivy"
      ],
      "metadata": {
        "id": "NFzo4-7cmFw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image_for_ivy_model(image):\n",
        "    def load_and_preprocess_img(img, new_size, crop, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "        compose = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(new_size),\n",
        "                transforms.CenterCrop(crop),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=mean, std=std),\n",
        "            ]\n",
        "        )\n",
        "        img = compose(img)\n",
        "        img = img.unsqueeze(0).permute((0, 2, 3, 1))\n",
        "        return img.numpy()\n",
        "\n",
        "    import ivy\n",
        "    ivy.set_backend(\"torch\")\n",
        "    img = ivy.asarray(load_and_preprocess_img(image, 256, 224))\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "# from PIL import Image\n",
        "# image = Image.open(\"/content/cat.jpg\")\n",
        "# preprocessed_image = preprocess_image_for_ivy_model(image)\n",
        "\n",
        "# display(preprocessed_image)"
      ],
      "metadata": {
        "id": "-UkNghgnjyAH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "compilation_count = 1\n",
        "ivy_model=None\n",
        "def infer_1image_using_ivy_model(image):\n",
        "    def _inceptionNet_torch_weights_mapping(old_key, new_key):\n",
        "        W_KEY = [\"conv/weight\"]\n",
        "        new_mapping = new_key\n",
        "        if any([kc in old_key for kc in W_KEY]):\n",
        "            new_mapping = {\"key_chain\": new_key, \"pattern\": \"b c h w -> h w c b\"}\n",
        "        return new_mapping\n",
        "\n",
        "    def inceptionNet_v1(pretrained=True):\n",
        "        \"\"\"InceptionNet-V1 model\"\"\"\n",
        "        if not pretrained:\n",
        "            return GoogLeNet()\n",
        "\n",
        "        reference_model = GoogLeNet()\n",
        "        url = \"https://download.pytorch.org/models/googlenet-1378be20.pth\"\n",
        "        w_clean = ivy_models.helpers.load_torch_weights(\n",
        "            url,\n",
        "            reference_model,\n",
        "            raw_keys_to_prune=[\"num_batches_tracked\"],\n",
        "            custom_mapping=_inceptionNet_torch_weights_mapping,\n",
        "            )\n",
        "        display(\"calling model with weights!\")\n",
        "        return GoogLeNet(v=w_clean)\n",
        "\n",
        "\n",
        "    global compilation_count\n",
        "    global ivy_model\n",
        "    if compilation_count == 1:\n",
        "      compilation_count -= 1\n",
        "      ivy_model = inceptionNet_v1(pretrained=True)\n",
        "      ivy.set_backend('torch')\n",
        "      ivy_model.compile(args=(image,))\n",
        "\n",
        "\n",
        "    output, _, _ = ivy_model(image)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "51nr2yDzklX1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Torch"
      ],
      "metadata": {
        "id": "JjhwzZFhmg3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image_for_torch_model(image):\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )])\n",
        "    # torch_img = Image.open(\"/content/cat.jpg\")\n",
        "    torch_img = preprocess(image)\n",
        "    torch_img = torch.unsqueeze(torch_img, 0)\n",
        "\n",
        "    return torch_img\n"
      ],
      "metadata": {
        "id": "BloFkVoUnx1v"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to numpy\n",
        "img = torch_img.permute((0, 2, 3, 1)).detach().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "Oxiqa_81mufv",
        "outputId": "3143c36e-c01d-4e82-8ad0-d5b34f2b5d26"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d901f8b96924>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert to numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch_img' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "def torch_googlenet(pretrained=True):\n",
        "        \"\"\"InceptionNet-V1 model\"\"\"\n",
        "        if not pretrained:\n",
        "            return models.googlenet(pretrained=False)\n",
        "\n",
        "        reference_model = models.googlenet()\n",
        "        url = \"https://download.pytorch.org/models/googlenet-1378be20.pth\"\n",
        "        w_clean = torch.hub.load_state_dict_from_url(url, progress=True)\n",
        "        model = models.googlenet()\n",
        "        model.load_state_dict(w_clean)\n",
        "        return model\n",
        "\n",
        "# compiling model one time\n",
        "torch_model = torch_googlenet()\n",
        "torch_model.eval()\n",
        "\n",
        "def infer_1image_using_torch_model(image):\n",
        "    with torch.no_grad():\n",
        "        output_gt = torch_model(image)\n",
        "    return output_gt"
      ],
      "metadata": {
        "id": "gbOm-JbDmucb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Accuracy"
      ],
      "metadata": {
        "id": "w326vKjZnGQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def return_list_of_N_random_images(N):\n",
        "    def load_images_from_folder(folder_path, num_images=100):\n",
        "        image_paths = []\n",
        "        for root, _, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
        "                    image_paths.append(os.path.join(root, file))\n",
        "\n",
        "        if len(image_paths) == 0:\n",
        "            print(\"No images found in the provided folder.\")\n",
        "            return None\n",
        "\n",
        "        if num_images > len(image_paths):\n",
        "            num_images = len(image_paths)\n",
        "\n",
        "        random.shuffle(image_paths)\n",
        "        selected_image_paths = image_paths[:num_images]\n",
        "\n",
        "        images = []\n",
        "        for image_path in selected_image_paths:\n",
        "            try:\n",
        "                img = Image.open(image_path)\n",
        "#                 img_array = np.array(img)\n",
        "                images.append(img)\n",
        "#                 img.close()\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image: {image_path}. Skipping.\\nError: {e}\")\n",
        "\n",
        "        return images\n",
        "\n",
        "    folder_path = \"/content/images\"\n",
        "    num_random_images = N\n",
        "    random_images = load_images_from_folder(folder_path, num_random_images)\n",
        "    # Now you have a list 'random_images_as_arrays' containing 100 randomly selected images as numpy arrays\n",
        "    return random_images"
      ],
      "metadata": {
        "id": "rGAhdbAbnK4n"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_model_acc(output_logit, output_gt_logit):\n",
        "    accuracy = tf.keras.metrics.Accuracy()\n",
        "    model_acc = torch.tensor([])\n",
        "\n",
        "    for _ in range(0, 1):\n",
        "        output_logit = torch.where(output_logit[0] > 0.5, torch.tensor(1), torch.tensor(0))\n",
        "        output_gt_logit = torch.where(output_gt_logit[0] > 0.5, torch.tensor(1), torch.tensor(0))\n",
        "\n",
        "        accuracy.update_state(output_logit, output_gt_logit)\n",
        "        acc_value = accuracy.result().numpy()\n",
        "        model_acc = torch.cat((model_acc, torch.tensor([acc_value])))\n",
        "\n",
        "    return model_acc\n"
      ],
      "metadata": {
        "id": "-hv3f4VtnK2Y"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_2inferences_for_each_image_from(no_of_images=1):\n",
        "    image_sequence=return_list_of_N_random_images(no_of_images)\n",
        "    for image in image_sequence:\n",
        "        display(image.size)\n",
        "        image1 = preprocess_image_for_ivy_model(image)\n",
        "        output_logit = infer_1image_using_ivy_model(image1)\n",
        "\n",
        "        image2 = preprocess_image_for_torch_model(image)\n",
        "        output_gt_logit = infer_1image_using_torch_model(image2)\n",
        "\n",
        "        model_acc = find_model_acc(output_logit, output_gt_logit)\n",
        "    display(model_acc)"
      ],
      "metadata": {
        "id": "mYvR36fEnK0O"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%timeit\n",
        "run_2inferences_for_each_image_from(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "BO6vmZzOnKxi",
        "outputId": "f726c873-7887-4e60-f8d1-874b9091a240"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(1918, 1280)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tensor([0.7500])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wcoR_25g0sR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s0mxOrND0sP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YqrRhj4z0syg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}